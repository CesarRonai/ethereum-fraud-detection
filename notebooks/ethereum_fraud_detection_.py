# -*- coding: utf-8 -*-
"""Ethereum Fraud Detection .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xdvTTLsfWruSffVL2o2k-UWAL2k4kpXH
"""

import sys
print("Python Version:",sys.version)

# Bibliotecas principais
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Machine Learning
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# Visualiza√ß√£o
sns.set(style="whitegrid")

# Carregar o dataset
df = pd.read_csv('/content/transaction_dataset.csv')
df.head()

# Dimens√£o do dataset
print("Shape:", df.shape)

# Tipos de dados e valores nulos
df.info()

# Estat√≠sticas descritivas das vari√°veis num√©ricas
df.describe()

# Verificar valores ausentes
df.isnull().sum()

# ‚úÖ Contagem das classes
df['FLAG'].value_counts()

# ‚úÖ Visualizar em gr√°fico
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(6,4))
sns.countplot(x='FLAG', data=df, palette='Set2')
plt.title('Distribui√ß√£o da Classe Alvo (FLAG)')
plt.xticks([0,1], ['Leg√≠tima (0)', 'Fraude (1)'])
plt.ylabel('Quantidade')
plt.xlabel('Classe')
plt.show()

# Remover espa√ßos extras dos nomes das colunas
df.columns = df.columns.str.strip()

# 1. Remover colunas irrelevantes
df_clean = df.drop(columns=['Unnamed: 0', 'Index', 'Address'])

# 2. Remover colunas categ√≥ricas com muitos valores nulos ou sem valor para o modelo
df_clean = df_clean.drop(columns=[
    'ERC20 most sent token type',
    'ERC20_most_rec_token_type'
])

# 3. Tratar valores ausentes (vamos preencher com 0, pois maioria s√£o valores financeiros nulos)
df_clean = df_clean.fillna(0)

# 4. Separar X e y
X = df_clean.drop(columns=['FLAG'])  # Features
y = df_clean['FLAG']                 # Target

# 5. Dividir treino e teste
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Verificar shapes
print("Shape X_train:", X_train.shape)
print("Shape y_train:", y_train.shape)

# Importar bibliotecas
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Criar o modelo XGBoost padr√£o (sem balanceamento)
model_base = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)

# Treinar com os dados desbalanceados
model_base.fit(X_train, y_train)

# Fazer previs√µes
y_pred_base = model_base.predict(X_test)

# Avaliar o desempenho
print("üìä Matriz de Confus√£o (Sem Balanceamento):")
print(confusion_matrix(y_test, y_pred_base))

print("\nüìà Relat√≥rio de Classifica√ß√£o (Sem Balanceamento):")
print(classification_report(y_test, y_pred_base, target_names=['Leg√≠tima', 'Fraude']))

from imblearn.over_sampling import SMOTE

# Instanciar o SMOTE
smote = SMOTE(random_state=42)

# Aplicar o SMOTE no conjunto de treino
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Verificar nova distribui√ß√£o das classes
import pandas as pd
print("Distribui√ß√£o ap√≥s SMOTE:")
print(pd.Series(y_resampled).value_counts())

# Criar o modelo XGBoost novamente
model_smote = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)

# Treinar com os dados balanceados
model_smote.fit(X_resampled, y_resampled)

# Prever no conjunto de teste original
y_pred_smote = model_smote.predict(X_test)

# Avaliar
print("Matriz de Confus√£o (com SMOTE):")
print(confusion_matrix(y_test, y_pred_smote))

print("\nRelat√≥rio de Classifica√ß√£o (com SMOTE):")
print(classification_report(y_test, y_pred_smote, target_names=['Leg√≠tima', 'Fraude']))

from sklearn.model_selection import RandomizedSearchCV
from xgboost import XGBClassifier
import numpy as np

# Espa√ßo de busca de hiperpar√¢metros
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 4, 5, 6, 7],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'gamma': [0, 1, 5],
    'reg_lambda': [0.1, 1, 10]
}

# Instanciar o modelo base
xgb_base = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)

# Configurar RandomizedSearch
random_search = RandomizedSearchCV(
    estimator=xgb_base,
    param_distributions=param_grid,
    n_iter=30,  # N√∫mero de combina√ß√µes a testar
    cv=3,       # Valida√ß√£o cruzada
    scoring='f1',  # Foco em F1-score (bom para classe desbalanceada)
    verbose=1,
    random_state=42,
    n_jobs=-1
)

# Executar busca
random_search.fit(X_resampled, y_resampled)

print("Melhores hiperpar√¢metros encontrados:")
print(random_search.best_params_)

# Criar o modelo final com os hiperpar√¢metros otimizados
xgb_final = XGBClassifier(
    subsample=0.8,
    reg_lambda=0.1,
    n_estimators=300,
    max_depth=7,
    learning_rate=0.2,
    gamma=0,
    colsample_bytree=1.0,
    eval_metric='logloss',
    random_state=42,
    use_label_encoder=False
)

# Treinar com os dados balanceados (SMOTE)
xgb_final.fit(X_resampled, y_resampled)

# Prever com o conjunto de teste original
y_pred_final = xgb_final.predict(X_test)

# Avalia√ß√£o final
from sklearn.metrics import classification_report, confusion_matrix

print("üìä Matriz de Confus√£o (Modelo Otimizado):")
print(confusion_matrix(y_test, y_pred_final))

print("\nüìà Relat√≥rio de Classifica√ß√£o (Modelo Otimizado):")
print(classification_report(y_test, y_pred_final, target_names=['Leg√≠tima', 'Fraude']))

import matplotlib.pyplot as plt
from xgboost import plot_importance

# Tamanho do gr√°fico
plt.figure(figsize=(12, 8))

# Plotar as 20 vari√°veis mais importantes
plot_importance(xgb_final, max_num_features=20, importance_type='gain')
plt.title('Import√¢ncia das Vari√°veis (por Gain)')
plt.show()

!pip install joblib  # Execute apenas se ainda n√£o tiver
import joblib

# Salvar o modelo final otimizado
joblib.dump(xgb_final, 'xgboost_fraude_modelo.pkl')

from google.colab import files

files.download('xgboost_fraude_modelo.pkl')